\chapter{Discussion}
\label{chap:discussion}

% \section{Calibration}
% \label{sec:discussion:calibration}

% % Ga Ka where Ka2 is not visible
% % point of the paragraph: empirical view is good enough
% In \cref{fig:theory:GaAs30keV-K-lines} the calibrated peak of Ga $K\alpha$ is directly on $K\alpha_1$ and the $K\alpha_\textnormal{HyperSpy}$, the emission from $K\alpha_2$ is missing.
% The energy difference is too small to differentiate the two peaks in EDS, but the peak has no left-shift, implying that the peak is not a mixture of $K\alpha_1$ and $K\alpha_2$.
% The peak shape is \brynjar{calculate the shape}, which is close to a perfect Gaussian.
% There are possible theoretical explainations for the lacking emission from $K\alpha_2$.
% One explaination is the quantum mechanical effect called \dots, where two close lines appear as one stronger line.
% This effect is not only present in EDS, but also in \dots.
% Another explaination is that Ga $K\alpha_2$ could also be weaker than $K\alpha_1$, but there is no reason for this to be the case.
% The take away from this is that the empirical view tends to be good enough for EDS analysis.


% \section{Introduction}
% \label{sec:discussion:intro}
The discussion is presented in this chapter.
Producing code from scratch is a time-consuming process, but it is also a learning process.
While developing the code, the author learned a lot about EDS analysis and new ideas emerged.
The sections below follow the structure of the sub-problems of the main problem statement formulated in \cref{chap:introduction}.

%
%


%
%
\section{Analysis steps in HyperSpy}
\label{sec:discussion:steps}
% What are done with the data at the different steps in the analysis when using HyperSpy?

\hyperref[subproblem1]{Sub-problem~\ref*{subproblem1}} was to do qualitative analysis of the GaAs wafer in AZtec and HyperSpy.
The analysis was done as an out-of-the-box analysis, i.e. just following the steps in the documentation.
AZtec has a GUI for analysis, but it is not possible to see what is done with the data at the different steps.
HyperSpy have documentation online, and the following subsections explain how the analysis was done in HyperSpy.

% The next sub-problem was to find out what is done with the data at the different steps in the analysis when using HyperSpy.
% In these steps it is assumed that the user have done qualitative analysis and want to do quantitative analysis on a set of elements.
% The analysis in AZtec is done as a black box, so it is not possible to see what is done with the data at the different steps.
Each subsection starts with some code lines, followed by an explanation of what the lines do.
All variables inside crocodile need to be set by the user, e.g. \verb|<element_list>| would be set to \verb|['Ga', 'As']| for the GaAs wafer.
An example notebook with quantification of the GaAs wafer is attached in APPENDIX.
\brynjar{Make a notebook with GaAs quantification in HyperSpy, with the data somehow.}

\subsection{Loading the data and specifying the elements}
\label{sec:discussion:steps:load}
\begin{quote}
    \verb|s = hs.load(<filepath>, signal="EDS_TEM")|

    \verb|s.set_elements(<element_list>)|
\end{quote}

The first step in the analysis is to load the data as a HyperSpy \verb|signal| type, and specifying the signal as TEM.
The \verb|signal| type is a class in HyperSpy that contains the data and the metadata, and it has methods for analysis.
The \verb|signal| type must be specified as TEM, because the \verb|signal| type for SEM is very limited and does not have a method for quantification.
When using .emsa files from AZtec, as is done in this project, the metadata contains some relevant and some irrelevant information.
The information relevant later in this project is:
acceleration voltage, dispersion, zero offset, energy resolution Mn $K\alpha$,
After loading, it is possible to plot the data with \verb|s.plot()|.

Already at this point there is a big problem with using HyperSpy on the acquired data: the analysis methods are not implemented for SEM but for TEM.
As explained by Skomedal in her master's thesis \cite{skomedal_improving_2022}, the quantification with the Cliff-Lorimer method of TEM EDS data use approximations which are valid for thin samples.
The approximation is that the sample is thin enough to ignore absorption and fluorescence, which is a very crude approximation to use on bulk SEM EDS data.
Still, the results show that it is possible to quantify the elements in the GaAs wafer with the Cliff-Lorimer method and get plausible results, but also that the analysis breaks down occasionally. % Copilot: but the results should be taken with a grain of salt.
Implementing quantification from the SEM type signal in HyperSpy have been discussed on the repositories GitHub issues page\footnote{\url{https://github.com/hyperspy/hyperspy/issues/2332}}.
The topic have been discussed in 2020, 2017 and 2015.
People have been working on quantification of SEM EDS data, but the work have not been finished yet.

The quantification done with the CL method in HyperSpy on the three different calibrations in \cref{tab:results:calibration-quantification} are all within 10\% of the expected composition.
Carter and Williams \cite[p. 612 and 648]{carter2016transmission} state that the accuracy of EDS is at best $\pm$ 3-5\%, but that it could be reduced to around $\pm$ 1.7\% with very long acquisition times and a careful analysis.
Carter and Williams claim that quantitative errors less than $\pm$ 5-10\% takes a lot of time and effort, and compositions with error below 5\% should be regarded extra carefully and with suspicion.
The best result, i.e. the result closest to 50\%, from the HyperSpy CL quantification is strangely from the AZtec calibration with a 6.25\% error.
It is strange that the AZtec calibration is the best, because that is the calibration which misses most on the line accuracy in \cref{tab:results:calibration-peak-accuracy}.
This implies that the calibration is not the most important factor for the quantification, at least not when the calibration is around $\pm$ 1\%.
The different calibrations are discussed further in \cref{sec:discussion:calibration}.
In the 30 kV GaAs sample the signal-to-noise ratio is good, the sample time was long and the peaks are well resolved, which make the input data good and allows for better quantification.
The CL method is known to work on SEM data, but normally one would also do the ZAF correction, which is described in chapter 19.10.3 of Goldstein \cite{goldstein_scanning_2018}.
The ZAF correction adjust the signal for the atomic number effect, the absorption in the bulk of the sample and the X-ray fluorescence of new lines with lower energy than the initial characteristic X-ray.
Since Ga and As are number 31 and 33 in the periodic table, it could be that the ZAF corrections are small and would not have a big effect on the results.
When doing the quantification it was tested to include O and C, but that changed the results from plausible to completely wrong.
The quantification analysis also broke down when using the 10 and 15 kV spectra, but the reason for that was not clear.
The breaking down of the analysis is discussed further in \cref{sec:discussion:failure}.


Results in this project show that the CL method in HyperSpy for TEM EDS data sometimes yield plausible results on SEM EDS data and sometimes break down, thus the method should be used with caution.
The goal of this project was not to do or to implement cutting edge quantification of EDS data from a SEM sample, but rather to understand the analysis steps and what factors that could affect the results.
Thus, using the quantification method from the TEM signal type was regarded as good enough for this project.





\subsection{Removing the background linearly}
\label{sec:discussion:steps:background}
\begin{quote}
    \verb|bw = s1.estimate_background_windows(windows_width=<number>)|

    \verb|iw =  s1.estimate_integration_windows(windows_width=<number>)|
\end{quote}

The next step is to remove the background, which with the above code is done by a linear fit.
The background can be removed through model fitting, which is covered in \cref{sec:discussion:steps:model_fitting}.
The variable \verb|windows_width| sets how wide the windows are for the background and integration, measured in FWHMs.
A good starting value for \verb|windows_width| is 2, but it should be tested by the user with a plot to see if the background will be removed correctly.
The estimated windows can be plotted with:

\begin{quote}
    \verb|s.plot(xray_lines=True, background_windows=bw, integration_windows=iw)|
\end{quote}


Plotting has proven to be a very useful tool for understanding what the different steps in the analysis do.
Unfortunately, the implemented plotting in HyperSpy is limited.
For example, when plotting a modelled spectrum with Ga and As, the model contains all the peaks in the HyperSpy library and all the peaks are plotted, but it is not possible to see what the different peaks are.
In other words, the plot of a model can show all the independent components, but adding a legend is not an option.
Another limitation of the plotting is the interactivity when saving the plot.
Matplotlib requires the code to run in the background for it to be interactive, while Plotly can save the plot as an interactive HTML file for later inspection.
One of the advantages with the plotting in HyperSpy is how accesable it is to plot in the different steps, which the user can use to understand and verify the analysis.
The HyperSpy plotting was probably not made with the intention of being used for publication, but rather for quick and easy plotting of the data.
If that is the case, the plotting serves its purpose well and has proven to be very useful for this project.
% Copilot did this: One of the authors of HyperSpy, Tomas Ostasevicius, has been working on a new plotting library for HyperSpy, which will be released in the next major release of HyperSpy.

One of the more helpful insights from plotting in this project was that removing the background is not trivial.
This is especially true for background signals modelled as a polynomial.
Linear background removal works well for some peaks, for example the K-lines of Ga and As, but not for the L-lines.
The background around the K-lines are more linear, because of the exponential decay of the background.
Around the L-lines the background is affected by the absorption of the Bremsstrahlung lower than ~2 keV and the observation of higher background levels below the highest peak in the spectrum.
The background removal is discussed further in \cref{sec:discussion:background}.




\subsection{Quantification after linear background removal}
\label{sec:discussion:steps:quantification:linear}

\begin{quote}
    \verb|s_i = s.get_lines_intensity(background_windows=bw, integration_windows=iw)|

    \verb|k_factors = [<k-factor 1>, <k-factor 2>]  |

    \verb|quant = s.quantification(s_i, method='CL', factors=k_factors)|

    \verb|print(f'E1: {quant[0].data[0]:.2f} \%, E1: {quant[1].data[0]:.2f} \%')|
\end{quote}

The quantification is done with the four lines of code above, where the last one prints the results.
The first line gets the intensity of the peak corresponding to the lines of the specified element.
HyperSpy selects automatically which lines to use for quantification.
To see which lines are used, the \verb|s_i| variable can be printed.
The second line sets the k-factors.
The k-factors in this project have been the one from AZtec, which are theoretically estimated.
The third line does the quantification, where the method is specified.
The method is the Cliff-Lorimer method, described in detail in Mari Skomedal's master thesis \cite[Sec. 2.2.3]{skomedal_improving_2022}.
HyperSpy has a method for quantification with the zeta factor method.
The zeta method requires the value for the beam current, which was not measured in this project. \footnote{Results from the zeta method can be converted to the cross section method, see the page "EDS Quantification" in the HyperSpy documentation.}
% \footnote{Results from the zeta methon can be converted to the cross section method, see \url{http://hyperspy.org/hyperspy-doc/current/user_guide/eds.html#eds-quantification}.}


% k-factors
The k-factors are essential for the Cliff-Lorimer quantification, and the k-factors listed in \cref{tab:results:k-factors} give some insight into how AZtec calculates the k-factors.
From the results it is clear that AZtec at least adjust the k-factor for element and for voltage.
It could be that factors like the beam current and time live could affect the k-factor, but this was not investigated in this project.
The L-line k-factors are start at 1.1 for 5 kV and increase to 1.2 for 15 kV.
The As k-factor for the L-line at 5, 10 ad 15 kV is 11\%, 7\% and 6\% greaten than for Ga.
The K-line k-factor are 3.3 for Ga and 4.2 for As, implying either a high sensitivity for higher voltages or two different models for calculating the k-factors.
Another possibility is that the K-lines and L-lines have a seperate model for calculating the k-factors.
This can be tested by extracting the k-factors for the K-lines from the 15 kV spectrum, as the 15 kV spectrum do have peaks for both the K-lines and L-lines.
The k-factors for Ga are in all four cases higher than for As, which makes sense, as all the Ga lines are higher and have more counts than the As lines.
Quantification of the ratio between the Ga and As peaks are dont in \cref{tab:results:ratios}, where it is clear that the k-factor push the ratio towards 1.
The peak ratio times the k-factor for 5 kV is 1.1, and the next closest to 1 is 1.3 for 30 kV.
This result imply that the theoretically calculated k-factors could be better for high and low voltages.


\ton{I am happy with the discussion untill this point (08.12.22). How would you rate it?}


% double peaks in sum, why to use modeled peaks and not raw counts.


\subsection{Removing the background with model fitting}
\label{sec:discussion:steps:model_fitting}
Another way to remove the background is to fit a model to the data.
This step would be done right after loading the data.
If the raw data contains a zero peak, as is the case for most Oxford instrument EDS detectors, the zero peak needs to be removed before fitting the model.
The zero peak is removed by skipping the first n channels, where n=30 works well with the data from the GaAs wafer.
The model fitting is done with the following code:

\begin{quote}
    \verb|s = s.isig[<zero_peak>:]|

    \verb|m = s.create_model(auto_background=False)|

    \verb|m.add_polynomial_background(order=12)|

    \verb|m.add_family_lines(<list_of_element_lines>)|

    \verb|m.plot()|

\end{quote}

The lines above removes the zero peak, create a model from the \verb|signal| s, adds a 12th order polynomial, add the lines of the elements in the \verb|signal|, and plot the model.
This model is not fitted, it is just a generated spectrum with the lines of the elements.
Eventually, the method \verb|create_model()| can take the boolean argument \verb|auto_add_lines=True|, which will automatically detect the elements in the sample.
The model consists of a number of components, which can be accessed with \verb|m.components|.
The components are all the Gaussian peaks in the spectrum, in addition to the background as a 12th order polynomial.
The order of the polynomial can be changed, but it should be tested by the user to see if it is a good fit.
Further, the model must be fitted.

\begin{quote}
    \verb|m.fit()|

    % \verb|m.fit_background()| % not neccessary

    \verb|m.plot()|
\end{quote}

The first line fits the model to the data to the components and the second line plots the model.
HyperSpy have an own option for fitting only the background.
Since the background is one of the components in m, it is fitted with the code line above.


\subsection{Quantification after model fitting}
\label{sec:discussion:steps:quantification:model}

\begin{quote}
    \verb|m_i = m.get_lines_intensity()|

    \verb|k_factors = [<k-factor 1>, <k-factor 2>] |

    \verb|quant = s.quantification(s_i, method='CL', factors=k_factors)|

    \verb|print(f'E1: {quant[0].data[0]:.2f} \%, E1: {quant[1].data[0]:.2f} \%')|

\end{quote}

The quantification after model fitting is done in the same way as in \cref{sec:discussion:steps:quantification:linear}, but with intensity from the model instead of the signal.
When modelling GaAs, the model can add the intensity from both K-lines and L-lines.
Since AZtec only gives the k-factors for either the K-lines or the L-lines, the user must remove the lines without k-factors before quantification.


\subsection{Calibrating the spectrum with the HyperSpy model}
\label{sec:discussion:steps:HyperSpycalibration}

\begin{quote}

    \verb|m.calibrate_energy_axis(calibrate='scale')|

    \verb|m.calibrate_energy_axis(calibrate='offset')|

\end{quote}

The two lines above calibrates the spectrum with the HyperSpy model and updates the dispersion and zero offset.
The metadata in the \verb|signal| s is updated with the new calibration.
Thus, doing the previous step with quantification after model fitting can give a more correct quantification.
%%% THE Calibration gave 1% better result, i think. I need to look at it again.
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%
%
\section{Peak and background modelling}
\label{sec:discussion:modelling}
% How can the peaks and the background be modelled in a way that is easy to understand?

The next sub-problem was to find out how the peaks and the background are modelled in a way that is easy to understand.
The model was built without HyperSpy, with the idea of making every step easier to understand.
The model was used to be able to remove the background and be able to calibrate the spectrum.
The model was compared to the HyperSpy model.
The model could be used to quantify the elements in the sample, but this was not done in this project.
\brynjar{Do I want to do this?}

% identyfy peaks with peak finder
% problem: double peaks, eg. Mo
% problem: includes zero peak. Is that an issue?
The first step in creating a model is to identify the peaks.
The peaks are assumed to be Gaussian curves.
The initial way of identifying peaks was that the user manually identified the peaks.
Later the peaks were identified with the function \verb|find_peaks()| from the \verb|scipy.signal| package.
Different peak prominence were tested, and the peak prominence of 0.01 gave the best results.


% make Gaussian and a polynomial
% problem: initial background guess. Solution: clip out the peaks with linear interpolation and fit a polynomial to the rest.
% problem: normalized data. initial guesses are hard on counts, and fitting is slower.
% early problem, now solved: some Gaussians would be put as the background. solution: fit both Gaussians and polynomial
The second step is to make a Gaussian in each peak and one polynomial for the background.
To do the fitting, the components need an initial guess.
The background needs a coefficient for each order of the polynomial.
Each Gaussian need to have a mean, a standard deviation, and a height.
The mean is the peak position.
The standard deviation is the width of the peak, where $\textnormal{FWHM} = \textnormal{std}*2*\sqrt{2*\ln{2}}$\footnote{FWHM defined at: \url{https://en.wikipedia.org/wiki/Full_width_at_half_maximum}}.
The height is the amplitude of the peak.
The easiest way to get the initial guesses for the Gaussians is to normalize the data and set all three parameters to 1.
In the normalization the highest peak was set to 1, and the rest of the peaks were scaled accordingly.
The best way to get the initial guesses for the background is to clip out the peaks with linear interpolation and fit a polynomial.
The initial guesses for the background are then the coefficients of the polynomial.
With the initial guesses, the whole model is ready to be fitted.

% curve fit
% problem: sometimes fails
The third step is to fit the model to the data.
Using the \verb|curve_fit()| function from the \verb|scipy.optimize| package, the model is fitted to the data.
The function \verb|curve_fit()| uses the Levenberg–Marquardt algorithm to fit the model to the data.
The function \verb|curve_fit()| returns the optimal parameters for the model.
Fitting both the Gaussians and the background at the same time makes the fitting more stable.
One of the first iterations, where the user manually inputted the peaks, the fitting tended to partially fail.
The issue was that the fitting only was done on the peaks.
To minimize the error in the fitting, one of the Gaussian curves with a low amplitude was moved and got a huge standard deviation, which compensated the background.
This was fixed by fitting both the Gaussians and the background at the same time.
Doing this made the fitting both better, and it failed less often.

\brynjar{Issue: fitting e.g. Mo with two clear peaks, but not with enough prominence to be found by the peak finder.}
%
%
\section{Calibration}
\label{sec:discussion:calibration}
% How is the spectrum calibrated, and is AZtec different than HyperSpy?

The next sub-problem was to calibrate the data with a self produced Python script.
With a fitted model of the spectrum, the calibration can be done.
Calibration can both be done on raw data with channels on the x-axis and on poorly calibrated data with energy on the x-axis.
The dispersion is calculated with \cref{eq:theory:calibration:dispersion}.
Table \cref{tab:results:calibrations} shows calibration from AZtec, HyperSpy, and the self produced Python script.


% Something about how the calibrated peak of Ga $K\alpha$ is directly on $K\alpha_1$.

%
%
\section{Background models}
\label{sec:discussion:background}
% How does different background models affect the quantitative analysis done in HyperSpy?

The next sub-problem was to find out how different background models affect the quantitative analysis done in HyperSpy, and how well different order polynomials fit the background.
The background models were tested on the spectrum of GaAs, and later also on \brynjar{TODO: other spectra. Also make a table here with results}.
The background was modelled as a polynomial of different orders.
To quantify the different background models, the residuals were calculated.
The residuals are the difference between the data and the model.
\brynjar{Use root-mean-square error?}
The TABLE XXXX \brynjar{make table} shows the residuals for the different order background models.
The best orders were visually inspected.
A later idea was to model the background as a spline, which is a piece wise polynomial.
The spline is a piece wise polynomial with a smooth transition between the pieces.
The spline was not tested in this project, but it could be a good alternative to the polynomial background model.

% TODO: Try the background as a spline, which is a piecewise polynomials.

% conclusion: try splines, but then what to do under the peak?

%
%
\section{Analysis failure}
\label{sec:discussion:failure}
% When does the analysis fail, both in AZtec and HyperSpy?

The next sub-problem was to find out when the analysis fails, both in AZtec and HyperSpy.
% quantification with 


\ton{Section about normalization too?}





\section{Calibration decision}

% Why I selected the Ga La and As Ka peaks for calibration.
% Less extrapolation. Peaks are far apart.
% The peaks need a good Gaussian fit.
% Need a nice curve.
% High peak to background.
% Should be a sample that is easily available. Cu tape would be nice, since it is in all labs and samples.
% Mo is far apart, but looks bad.
% Mo is also harder to fit automatically, because of the close double peak.



\section{Choices in HyperSpy}

% Using EDS_TEM since EDS_SEM does not have all these functions.
% Commercial packages do quantify SEM signal, and they are kinda good at it.
% Should be more accurate on NW sample, but is it?
% Reference the discussion on GitHub?





