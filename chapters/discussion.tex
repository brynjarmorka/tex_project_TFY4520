\chapter{Discussion}
\label{chap:discussion}


% % Ga Ka where Ka2 is not visible
% % point of the paragraph: empirical view is good enough
% In \cref{fig:theory:GaAs30keV-K-lines} the calibrated peak of Ga K$\alpha$ is directly on K$\alpha_1$ and the K$\alpha_\textnormal{HyperSpy}$, the emission from K$\alpha_2$ is missing.
% The energy difference is too small to differentiate the two peaks in EDS, but the peak has no left-shift, implying that the peak is not a mixture of K$\alpha_1$ and K$\alpha_2$.
% The peak shape is \brynjar{calculate the shape}, which is close to a perfect Gaussian.
% There are possible theoretical explainations for the lacking emission from K$\alpha_2$.
% One explaination is the quantum mechanical effect called \dots, where two close lines appear as one stronger line.
% This effect is not only present in EDS, but also in \dots.
% Another explaination is that Ga K$\alpha_2$ could also be weaker than K$\alpha_1$, but there is no reason for this to be the case.
% The take away from this is that the empirical view tends to be good enough for EDS analysis.


The discussion is presented in this chapter.
Producing code from scratch is both a time-consuming process and a learning process.
While developing the code, the author learned a lot about EDS analysis and  ideas emerged for how EDS analysis could be improved.
This chapter begins with an initial discussion of the qualitative results, and then it connects the sub-problems to the results and work done in the project.
The order of the sub-problems is the same as in \cref{chap:introduction}, which is based on the work process.
The chapter ends with a discussion of the \hyperref[mainproblem]{Main problem}.



\section{General results from the spectra}
\label{sec:discussion:general}

This section starts with peak intensities, then the background, and finally some strays observed.


\subsection{Peak intensities}
\label{sec:discussion:general:intensities}

% singal-to-noise ratio
All the spectra have peaks with high peak-to-background ratio, at least for the main peaks. %discuss: sampling good enough? I do not know if i miss some peaks, but the ones i have are nice for qualitative analysis at least.
Good sampling settings give a high signal-to-noise ratio, which is important for analysis.
The high peaks in the spectrum are well-defined and fit the Gaussian curves well.
These peaks are good enough for qualitative analysis, but there are some smaller signals which have been harder to interpret.
A small signal can in some cases be interpreted in a qualitative analysis but is hard to use properly in a quantitative analysis.
However, interpreting every small signal as radiation from a specific element is not always correct.
The small signal of Fe K$\alpha$ in the FIB stub spectrum is an example of this, where there is a 20\% increase of counts to the right of the Mn K$\alpha$ peak.
It might be that this signal is from Fe, but it might also just be noise or an artifact.
\cref{tab:theory:Ga-lines} list the Ga-lines and their weights, and it is clear that some lines have very low weights.
In this project it was decided not to use a hard threshold for the signal-to-noise ratio, which can be helpful in some cases.
Setting a hard threshold should be based on both literature and empirical data for the specific detector setup.
A danger with setting a hard threshold is that it can be too strict, which can lead to missing peaks of elements which are low in concentration.
If the sampling settings were not good enough, some peaks can be missing in the spectra.
One example of this is in the NW spectrum: the low As K$\beta$ peak in the 15 kV spectrum has an overvoltage of ~3 keV and is visible, but the Cu K$\alpha$ peak in the 10 kV spectrum with overvoltage of ~2 keV is not visible.
Multiple data sets on the same area can be used to verify the sampling settings, since missing peaks in one data set can be visible in another data set.
In general, the signals in the spectra are best in the 30 kV spectra.
In conclusion, it is assumed that the sampling settings were good enough.


% The zero peak from the Oxford detector is visible in all the spectra as the first peak, with a center at 0.00924 keV.

% height of the peaks in relation to the settings
The heights, or intensities, of the peaks are varying a lot.
The plots of the spectra in \cref{fig:results:Spectra_Al,fig:results:Spectra_NW} are normalized, but the total counts and maximum counts are included in \cref{tab:method:detector:settings}.
No direct pattern between the amount of counts and the settings are apparent.
Trends observed are that higher counts are obtained by increasing the current or the acceleration voltage, but this also increases the dead time.
To figure out a pattern, fewer variables should be changed at the same time.
This was done for the nanowire sample, where the I$_{beam}$ and live time was kept constant.
Still, a direct pattern is not apparent.
As stated in Goldstein, "high counts and stable peak structures are critical for successful peak intensity measurements", especially to identify minor and trace constituents \cite[page 318]{goldstein_scanning_2018}.
The relation between the counts and settings could be further investigated to figure out which settings yield the best spectra in terms of good count statistics.

% the highest peaks, 
The peak with the highest count is in all the spectra below 5 keV.
For the spectra with strong K and L peaks from the same element, i.e. the lines of Ga, As, and Mo, the L-peaks are more intense than the K-peaks.
In the Mo 30 kV spectrum, the K$\alpha$ peak is only 2\% the height of the L$\alpha$ peak.
This low intensity of the K$\alpha$ peak is partially due to lower overvoltage, since the peak is above U/2.
For the Ga and As the relative intensity of the 30 kV spectra in the bulk and nanowire is not similar.
The K-lines in the bulk sample is around 2 times as intense as the K-lines in the nanowire sample.
Both spectra are normalized to the L-line of Ga.
It was expected that the nanowire would give lower intensity, which is why the
Using \cref{tab:results:ratios} with the ratios of the K-lines to the L-lines, it is clear that even the ratios of Ga and As lines change in the different spectra.
When applying the theoretically estimated k-factor for the respective spectrum, the ratios get closer to 1, but there is still a spread.
This implies that the settings for acquiring spectra is important for the intensity of the peaks, which again is important for the quantitative analysis.
Investigation of which settings yield better spectra should improve the accuracy of the quantitative analysis.


% Missing Cu La peak
Even though most of the spectra appear to have good intensities and good signal-to-noise ratio, there are some strange results.
From the results it is apparent that the Cu-tape is giving a bad Cu signal and should not be used as a Cu reference.
The strongest signal from the Cu-tape is from C, which is probably from the sticky tape material.
Regardless, the strange result is the completely missing signal from Cu L$\alpha$ in the Cu-tape 30 kV spectrum.
This spectrum has a clear signal from Cu K$\alpha$ and Cu K$\beta$, but no signal from Cu L$\alpha$ at all.
In general the L-peaks are stronger than the K-peaks.
In the 30 kV NW spectrum, Mo is giving a low but clear signal at the L$\alpha$ line, and even a small signal at the L$\beta$ and Ll lines, but no signal from the Mo K$\alpha$ line.
This is not strange as even the K-line signal from pure Mo is quite low.
The missing Cu L$\alpha$ signal is still strange when comparing to the NW spectra, because the Cu signal in the NW spectra all have a clear signal from Cu L$\alpha$ which is around 30\% stronger than the Cu K$\alpha$ signal.
When taking the Cu spectra, the Cu-tape got visually deformed by the beam, which can be part of the explanation of the missing signal.
Finding both K-lines and L-lines is an important aspect of qualitative EDS analysis, because a present K-line without an L-line indicate that the identified element is not the correct one.
The result with the missing L-line show that, unfortunately, it is possible to acquire spectra where a K-line is present, but the L-line is missing.
Further, investigating this could reveal if this is an issue with the Cu-tape, the detector, the settings, or an artifact that can appear in other spectra.


% width of the peaks
From the spectra shown and \cref{tab:results:ratios} including the FWHM, it is clear that the peaks broaden with higher E.
Increase of the FWHM with E is expected, as explained in subsection 16.1.1 in Goldstein \cite{goldstein_scanning_2018}.
From the FWHM the resolution of the detector can be calculated with a conversion factor, since the resolution of EDS is defined as the FWHM of Mn K$\alpha$.
This was done by Skomedal, and was considered to be a part of this project too.
However, other parts of the project was prioritized.


One of the prioritized parts was to do calibrations, and it was quantified that the AZtec calibration was off, which might be connected to the center of the zero peak.
With the most accurate calibration, the zero peak did not have its center at 0 keV.
In the spectra the zero peak start before 0 keV and has its maximum at 0.008 keV.
Since AZtec is a black box, it is not known how the calibration is done.
If AZtec is using the zero peak to calibrate, this could be the reason for less accurate calibration.
Another possibility is that the calibration was done on installation, and that the detector has not been calibrated since then.
\ton{This paragraph it intended to say something about the zero peak and connect it to AZtec. Do you find it useful?}


%  overlapping peaks
A challenge which is unaffected by poor calibration is overlapping peaks.
Section 20.3 in Goldstein explains how to deal with overlapping peaks \cite{goldstein_scanning_2018}.
Dealing with overlapping peaks in model fitting is just modelling two Gaussian curves on top of each other.
In the self produced fitting, the Gaussians have three parameters: center, standard deviation and height.
Model fitting in HyperSpy is done with fixed centers, which means that the peak centers cannot change.
Calibration can affect the result if fitting is done with a model where the peak centers cannot change.
If the center of the smaller peak is off, larger peak will dominate the fitting and the smaller peak will get a lower intensity than it should.
This problem is not present in the self produced fitting, since the peak centers are free parameters.
An example of slightly overlapping peaks is the As K$\alpha$ and Ga K$\beta$ peaks in the NW and bulk spectra.
An example of severely overlapping peaks is the Mo L$\alpha$ and Mo L$\beta_1$ peaks in the Mo spectra.
The Mo L$\alpha$ peak has one of the highest error in the peak accuracy in \cref{tab:results:calibration-peak-accuracy}, where the calibrations are compared. The reason for this is that the Mo L$\alpha$ peak is overlapping with the Mo L$\beta_1$ peak, and the automatic peak finder used to find the peaks here cannot distinguish between the two peaks.
Thus, the severely overlapping peak is modelled as a single peak which has its center shifted slightly to the left.
That is why all the calibrations miss the Mo L$\alpha$ peak with 20 eV.
This issue was fixed manually when making the Mo L$\alpha$ and Mo K$\alpha$ calibration, and can with more time be fixed in the automatic peak finder.
\brynjar{Figure of overlapping peaks?}




























\subsection{The background}
\label{sec:discussion:general:background}
% the background with shape and intensity
Another problem in the self produced code that can improve with more time is fitting the background.
An idea that emerged towards the end of the project was to model the background as a spline.
A spline is a piece wise polynomial function connected in knots.
The fit of the background was observed to be bad at lower energies, where the background fit is more crucial for finding the right intensity of a peak.
It is more crucial, because the background change more with energy at lower energies.
The first iteration of the spline background would be to have the knot at the highest peak, because of the observation that the background decrease to around 50-10\% after the highest peak.


% highest peak effect on the background
All the spectra show the same behavior that the background is higher before the highest peak than after it.
In the 30 kV GaAs spectrum the background is at 2200 counts before the overlapping L-line peaks, and falls to 500 counts after the peaks.
In the 30 kV Si spectrum the background falls from 2000 to 200 counts after the main peak.
The background shape in Si 30 kV is almost linear from 0.6 to 1.6 keV ending at 2000 counts, then it drops to 200 after the peak at 1.6-1.9 keV.
After the drop the peak follow the expected shape, illustrated in \cref{fig:theory:background_illustration}.
In the 5 and 10 kV Si spectra the background falls from 2500 to 1000 counts after the main peak.
The Mo 10 kV spectrum, which is the spectrum with overall most background counts, has a background signal which falls from 5400 to 2900 counts after the main peak.
In the nanowire spectra, which in general have the lower background counts, the background falls around 50\% after the high L-peaks.
The reason for the higher background to the left of the highest peak is that the X-rays formed with energy above the highest peak can be absorbed and re-emitted with the energy of the line in the highest peak.
When the code for the background was written, the effect of the highest peak was not considered, which resulted in visible bad background fits at the lower energies.
This is shown in \cref{fig:results:fit_GaAs30kV}, where the fit in general is good but misses on the background between 0 and 2.5 keV.
This effect is changing the value of the background, and also its shape.


% background shape
The shape of the background is varying between the spectra, and is affected by the peaks and the acceleration voltage.
All the 5 kV spectra decrease more or less linearly from 1 to 5 keV.
This is due to the overvoltage, which lowers the possible background radiation to zero after 5 keV in the 5 kV spectra.
In the GaAs, Si and Mo the relative height of the background decrease with higher acceleration voltage.
The background in the Cu spectra increase with higher acceleration voltage.
The most similar background signal over different voltages are in the NW spectra. % discuss: thin sample -> less background?
This similarity is due to the fact that thin samples produce less background radiation than bulk samples.
In general, backgrounds in the acquired spectra are low, but with different shapes.

% There is nothing to "break" on??? Cu 5 kV and 10 kV with almost no background
% discussion: explain that the bg is dependent on a peak (ie material) being present. This makes fiting hard, since there are some peak-dependency which is hard to predict.
% discussion: nothing to reduce the speed of the e-


%   Need to account for or fit the background, to be able to substract it and get right peak counts.
%   Think how this representation and later discussion will link to set aim of the work ("better EDX").


% For example, the Cu 10 kV spectrum have its highest peak at 0.5 keV, and the background is almost zero above 0.5 keV. not the best example
% The Al spectrum background is high before the high Al K$\alpha$ peak, and much lower after the high peak.
% Both the value and the shape of the background is different before and after the peak.
% The same behavior is clearly true in the Si spectra.
% TON comment:
% yes, it is absorption at that energy.





\subsection{The strays and artifacts}
\label{sec:discussion:general:strays}

\brynjar{This subsection about stray are not finished.}
% discussion: understanding the strays properly is actually helpful for the qualitative analysis, since some elements can be excluded based on the strays and some strays are only present in certain materials.
% Si stray in all spectra
% stray in NW outside beam, Mo, Cu and Sb


TODO: include unknown peaks here


% TODO: Log-scale plots show strays better. Not used in the thesis, but could be used in the future. Is common.


% getting counts, but not coincidence peaks. Use:
% Dead time at around 30\% was recommended by the supervisor of this project, Antonius T. J. von Helvoort, and is well below the problematic dead time of >60\%.
% The dead time on the nanowire area was closer to 20\%, which is what Goldstein \cite[page 223]{goldstein_scanning_2018} recommends.


% TON There are many different strays. SOme were mentioned above. This paragraph is short. Maybe better to have it as a header in discussion?


In addition to the characteristic peaks and the background, there are also artifacts and strays in the spectra. % discussion: lower peaks are super important, eg in the Al spectra where it is small amounts of Mg and Mn, probably. Asserting what is a stray and what is characteristic is important for quantification.
All the spectra have signal from the K$\alpha$ line of C and O, which could be contamination and oxide layers. % discussion: this is contamination. Carbon deposited from the beam, visisble on the Cu-tape images. Oxide layers? If yes, look at material properties.
The C and O signal is higher at lower acceleration voltage.
All the spectra have a Si peak at 1.74 keV. % This is the Si escape peak for some spectra, but not all.
For all but the Si spectra, the Si K$\alpha$ peak at 1.74 keV is a stray from outside the beam or the Si escape peak from the detector.
Some spectra have additional signals from elements outside the area of the main beam, like the Mo, Sb and Cu peak in the NW spectra. % pretty sure it is Sb, because of the series of peaks at 3.6, 3.8, 4.1 and 4.3. 
% discuss both interactin volume and bonus peaks from X-ray strays in the chamber/sample.
The Sb peaks in the NW spectra are at 3.60, 3.85, 4.10 and 4.35 keV, being the L$\alpha_1$, L$\beta_1$, L$\beta_2$ and L$\gamma_1$ peaks.
All four Mo spectra have a peak at 0.175 keV, which match best with B K$\alpha$ at 0.183 keV.
\brynjar{Figure of strays?}


% Sum peaks, to discussion
% The Si spectra have a sum peak at 3.49 keV, which is the sum of Si K$\alpha$ at 1.74 keV. % discussion: this could be Sb, but not like NW where Sb is a series of peaks. Also say something about the resolution of the detector, which is higher at lower E. And also that Sb cannot be excluded bc. of the K peak. And also that the sum peaks can be tested with low DT. And something here about the bad calibration in AZtec which might make this sum peak as a Sb peak, but I now know that it is a Si peak sum peak and not a Sb peak.
% The Al spectrum have its highest peak at 1.48 keV and a sum peak at 2.98 keV
% The Mo spectrum have a sum peak at 4.65 keV, which is the sum of Mo L$\alpha$ at 2.293 keV and Mo L$\beta_1$ at 2.395 keV. % same shape as Mo La  Mo Lb1, i.e. also Mo La+MoLa sum
% The GaAs spectrum at 30 kV have two small sum peak signals at 18.5 and 19.5 keV, while the NW spectrum at 30 kV with lower DT does not have these peaks. % discussion: results show that sum peaks are lower with lower DT

% Si stray on Cu
% also a sum peak at Al 30 kV spectrum (look at DT)
% Si double count at 3.4 keV which could be Sn (but the E resolution could seperate them Ton thinks at this low keV and Sn would have multiple peaks, cannot exclude bc of the K peak could count with low DT to minimize double counts consequence of AZtecs bad calibration)


Cu 10 kV is almost only noise. Scaled badly?









\section{Quantification in AZtec}
\label{sec:discussion:az_quantification}
%How accurate is the out-of-the-box quantification in AZtec?

\hyperref[subproblem1]{Sub-problem~\ref*{subproblem1}} was to do quantification in AZtec.
The analysis in AZtec was done on the GaAs bulk data, and are given in \cref{tab:initial_quantification}.
The data was treated as both a SEM and a TEM signal, because of the limitations in HyperSpy, which are discussed in \cref{sec:discussion:steps}.
AZtec has a GUI for analysis, but it is not possible to see what is done with the data at the different steps.

For all four spectra the quantification was most accurate when treating the data as a SEM singal.
This was not unexpected, since the spectra in fact are SEM spectra.
It is interesting that the most accurate SEM quantification at 15 kV, while the most unaccurate TEM quantification at 15 kV.
In general, the quantification as SEM signals are very accurate, all withing 5\% of the correct value for Ga and As.
The AZtec and HyperSpy quantification on the 5 and 30 kV spectra are very similar, which could imply that they use the same method, i.e. the CL method.
However, the quantification on the 10 and 15 kV spectra are very different, because the HyperSpy quantification breaks down.
This break down in covered more in \cref{sec:discussion:failure}.

In general the quantification done on the correct type of signal is quite accurate in AZtec, but the problem is that the quantification is done as a black box.
Adjusting the parameters of for example the background fitting or the peak finder is not possible, and the user cannot see what is done with the data at the different steps.
A issue with this is visible in the initial AZtec plot in \cref{fig:GaAs30kV_AZ}, where the Ga K$\beta$ peak is not identified.
The peak is clearly visible in the unscaled spectrum, but the peak finder in AZtec does not find it.
Clicking on the peak opens a windows where the user can select what peak this would correspond to, but the only options are a selection of elements where the peak is obviously wrong.
Not being able to identify the peak that big should raise concern, especially since the peak is overlapping with the As K$\alpha$ peak and thus would affect the quantification.
Since the quantification process is hidden, the user does not know if AZtec deals with the overlapping peaks correctly.









%
%
\section{Analysis steps in HyperSpy}
\label{sec:discussion:steps}
% What are done with the data at the different steps in the analysis when using HyperSpy?

\hyperref[subproblem2]{Sub-problem~\ref*{subproblem2}} was to fiugre out what the steps are doing in the qualitative analysis in HyperSpy.
HyperSpy have documentation online, and the following subsections explain how the analysis was done in HyperSpy.

% The next sub-problem was to find out what is done with the data at the different steps in the analysis when using HyperSpy.
% In these steps it is assumed that the user have done qualitative analysis and want to do quantitative analysis on a set of elements.
% The analysis in AZtec is done as a black box, so it is not possible to see what is done with the data at the different steps.
Each subsection starts with some code lines, followed by an explanation of what the lines do.
All variables inside crocodile need to be set by the user, e.g. \verb|<element_list>| would be set to \verb|['Ga', 'As']| for the GaAs wafer.
An example notebook with quantification of the GaAs wafer is attached in APPENDIX.
\brynjar{Make a notebook with GaAs quantification in HyperSpy, with the data somehow.}

\subsection{Loading the data and specifying the elements}
\label{sec:discussion:steps:load}
\begin{quote}
    \verb|s = hs.load(<filepath>, signal="EDS_TEM")|

    \verb|s.set_elements(<element_list>)|
\end{quote}

The first step in the analysis is to load the data as a HyperSpy \verb|signal| type, and specifying the signal as TEM.
The \verb|signal| type is a class in HyperSpy that contains the data and the metadata, and it has methods for analysis.
The \verb|signal| type must be specified as TEM, because the \verb|signal| type for SEM is very limited and does not have a method for quantification.
When using .emsa files from AZtec, as is done in this project, the metadata contains some relevant and some irrelevant information.
The information relevant later in this project is:
acceleration voltage, dispersion, zero offset, energy resolution Mn K$\alpha$,
After loading, it is possible to plot the data with \verb|s.plot()|.

Already at this point there is a big problem with using HyperSpy on the acquired data: the analysis methods are not implemented for SEM but for TEM.
As explained by Skomedal in her master's thesis \cite{skomedal_improving_2022}, the quantification with the Cliff-Lorimer method of TEM EDS data use approximations which are valid for thin samples.
The approximation is that the sample is thin enough to ignore absorption and fluorescence, which is a very crude approximation to use on bulk SEM EDS data.
Still, the results show that it is possible to quantify the elements in the GaAs wafer with the Cliff-Lorimer method and get plausible results, but also that the analysis breaks down occasionally. % Copilot: but the results should be taken with a grain of salt.
Implementing quantification from the SEM type signal in HyperSpy have been discussed on the repositories GitHub issues page\footnote{\url{https://github.com/hyperspy/hyperspy/issues/2332}}.
The topic have been discussed in 2020, 2017 and 2015.
People have been working on quantification of SEM EDS data, but the work have not been finished yet.

The quantification done with the CL method in HyperSpy on the three different calibrations in \cref{tab:results:calibration-quantification} are all within 10\% of the expected composition.
Carter and Williams \cite[p. 612 and 648]{carter2016transmission} state that the accuracy of EDS is at best $\pm$ 3-5\%, but that it could be reduced to around $\pm$ 1.7\% with very long acquisition times and a careful analysis.
Carter and Williams claim that quantitative errors less than $\pm$ 5-10\% takes a lot of time and effort, and compositions with error below 5\% should be regarded extra carefully and with suspicion.
The best result, i.e. the result closest to 50\%, from the HyperSpy CL quantification is strangely from the AZtec calibration with a 6.25\% error.
It is strange that the AZtec calibration is the best, because that is the calibration which misses most on the line accuracy in \cref{tab:results:calibration-peak-accuracy}.
This implies that the calibration is not the most important factor for the quantification, at least not when the calibration is around $\pm$ 1\%.
The different calibrations are discussed further in \cref{sec:discussion:calibration}.
In the 30 kV GaAs sample the signal-to-noise ratio is good, the sample time was long and the peaks are well resolved, which make the input data good and allows for better quantification.
The CL method is known to work on SEM data, but normally one would also do the ZAF correction, which is described in chapter 19.10.3 of Goldstein \cite{goldstein_scanning_2018}.
The ZAF correction adjust the signal for the atomic number effect, the absorption in the bulk of the sample and the X-ray fluorescence of new lines with lower energy than the initial characteristic X-ray.
Since Ga and As are number 31 and 33 in the periodic table, it could be that the ZAF corrections are small and would not have a big effect on the results.
When doing the quantification it was tested to include O and C, but that changed the results from plausible to completely wrong.
The quantification analysis also broke down when using the 10 and 15 kV spectra, but the reason for that was not clear.
The breaking down of the analysis is discussed further in \cref{sec:discussion:failure}.


Results in this project show that the CL method in HyperSpy for TEM EDS data sometimes yield plausible results on SEM EDS data and sometimes break down, thus the method should be used with caution.
The goal of this project was not to do or to implement cutting edge quantification of EDS data from a SEM sample, but rather to understand the analysis steps and what factors that could affect the results.
Thus, using the quantification method from the TEM signal type was regarded as good enough for this project.





\subsection{Removing the background linearly}
\label{sec:discussion:steps:background}
\begin{quote}
    \verb|bw = s1.estimate_background_windows(windows_width=<number>)|

    \verb|iw =  s1.estimate_integration_windows(windows_width=<number>)|
\end{quote}

The next step is to remove the background, which with the above code is done by a linear fit.
The background can be removed through model fitting, which is covered in \cref{sec:discussion:steps:model_fitting}.
The variable \verb|windows_width| sets how wide the windows are for the background and integration, measured in FWHMs.
A good starting value for \verb|windows_width| is 2, but it should be tested by the user with a plot to see if the background will be removed correctly.
The estimated windows can be plotted with:

\begin{quote}
    \verb|s.plot(xray_lines=True, background_windows=bw, integration_windows=iw)|
\end{quote}


Plotting has proven to be a very useful tool for understanding what the different steps in the analysis do.
Unfortunately, the implemented plotting in HyperSpy is limited.
For example, when plotting a modelled spectrum with Ga and As, the model contains all the peaks in the HyperSpy library and all the peaks are plotted, but it is not possible to see what the different peaks are.
In other words, the plot of a model can show all the independent components, but adding a legend is not an option.
Another limitation of the plotting is the interactivity when saving the plot.
Matplotlib requires the code to run in the background for it to be interactive, while Plotly can save the plot as an interactive HTML file for later inspection.
One of the advantages with the plotting in HyperSpy is how accessible it is to plot in the different steps, which the user can use to understand and verify the analysis.
The HyperSpy plotting was probably not made with the intention of being used for publication, but rather for quick and easy plotting of the data.
If that is the case, the plotting serves its purpose well and has proven to be very useful for this project.
% Copilot did this: One of the authors of HyperSpy, Tomas Ostasevicius, has been working on a new plotting library for HyperSpy, which will be released in the next major release of HyperSpy.

One of the more helpful insights from plotting in this project was that removing the background is not trivial.
This is especially true for background signals modelled as a polynomial.
Linear background removal works well for some peaks, for example the K-lines of Ga and As, but not for the L-lines.
The background around the K-lines are more linear, because of the exponential decay of the background.
Around the L-lines the background is affected by the absorption of the Bremsstrahlung lower than ~2 keV and the observation of higher background levels below the highest peak in the spectrum.
The background removal is discussed further in \cref{sec:discussion:background}.




\subsection{Quantification after linear background removal}
\label{sec:discussion:steps:quantification:linear}

\begin{quote}
    \verb|s_i = s.get_lines_intensity(background_windows=bw, integration_windows=iw)|

    \verb|k_factors = [<k-factor 1>, <k-factor 2>]  |

    \verb|quant = s.quantification(s_i, method='CL', factors=k_factors)|

    \verb|print(f'E1: {quant[0].data[0]:.2f} \%, E1: {quant[1].data[0]:.2f} \%')|
\end{quote}

The quantification is done with the four lines of code above, where the last one prints the results.
The first line gets the intensity of the peak corresponding to the lines of the specified element.
HyperSpy selects automatically which lines to use for quantification.
To see which lines are used, the \verb|s_i| variable can be printed.
The second line sets the k-factors.
The k-factors in this project have been the one from AZtec, which are theoretically estimated.
The third line does the quantification, where the method is specified.
The method is the Cliff-Lorimer method, described in detail in Mari Skomedal's master thesis \cite[Sec. 2.2.3]{skomedal_improving_2022}.
HyperSpy has a method for quantification with the zeta factor method.
The zeta method requires the value for the beam current, which was not measured in this project. \footnote{Results from the zeta method can be converted to the cross section method, see the page "EDS Quantification" in the HyperSpy documentation.}
% \footnote{Results from the zeta methon can be converted to the cross section method, see \url{http://hyperspy.org/hyperspy-doc/current/user_guide/eds.html#eds-quantification}.}


% k-factors
The k-factors are essential for the Cliff-Lorimer quantification, and the k-factors listed in \cref{tab:results:k-factors} give some insight into how AZtec calculates the k-factors.
From the results it is clear that AZtec at least adjust the k-factor for element and for voltage.
It could be that factors like the beam current and time live could affect the k-factor, but this was not investigated in this project.
The L-line k-factors are start at 1.1 for 5 kV and increase to 1.2 for 15 kV.
The As k-factor for the L-line at 5, 10 ad 15 kV is 11\%, 7\% and 6\% greater than for Ga.
The K-line k-factor are 3.3 for Ga and 4.2 for As, implying either a high sensitivity for higher voltages or two different models for calculating the k-factors.
Another possibility is that the K-lines and L-lines have a separate model for calculating the k-factors.
This can be tested by extracting the k-factors for the K-lines from the 15 kV spectrum, as the 15 kV spectrum do have peaks for both the K-lines and L-lines.
The k-factors for Ga are in all four cases higher than for As, which makes sense, as all the Ga lines are higher and have more counts than the As lines.
Quantification of the ratio between the Ga and As peaks are shown in \cref{tab:results:ratios}, where it is clear that the k-factor push the ratio towards 1.
The peak ratio times the k-factor for 5 kV is 1.1, and the next closest to 1 is 1.3 for 30 kV.
This result implies that the theoretically calculated k-factors could be better for high and low voltages.




% overlapping peaks in sum, why to use modeled peaks and not raw counts.


\subsection{Removing the background with model fitting}
\label{sec:discussion:steps:model_fitting}
Another way to remove the background is to fit a model to the data.
This step would be done right after loading the data.
If the raw data contains a zero peak, as is the case for most Oxford instrument EDS detectors, the zero peak needs to be removed before fitting the model.
This requirement can easily cause problems and confusions, as this step is not very clear in the HyperSpy documentation.
The reason for this being not that clear, is probably that different detectors have different zero peaks, and the zero peak is not always at the same place.
The zero peak is removed by skipping the first n channels, where n=30 works well with the data from the GaAs wafer.
The model fitting is done with the following code:

\begin{quote}
    \verb|s = s.isig[<zero_peak_last_index>:]|

    \verb|m = s.create_model(auto_background=False)|

    \verb|m.add_polynomial_background(order=12)|

    \verb|m.add_family_lines(<list_of_element_lines>)|

    \verb|m.plot()|

\end{quote}

The lines above removes the zero peak, create a model from the \verb|signal| s, adds a 12th order polynomial, add the lines of the elements in the \verb|signal|, and plot the model.
This model is not fitted, it is just a generated spectrum with the lines of the elements.
Eventually, the method \verb|create_model()| can take the boolean argument \verb|auto_add_lines=True|, which will automatically detect the elements in the sample.
The model consists of a number of components, which can be accessed with \verb|m.components|.
The components are all the Gaussian peaks in the spectrum, in addition to the background as a 12th order polynomial.
The Gaussian components are based on the lines that the added elements in \verb|<element_list>| have.
If \verb|auto_background| is not set to false, HyperSpy will add a 6th order polynomial.
The order of the polynomial can be changed, but it should be tested by the user to see if it is a good fit.
6th order polynomials work, but as shown in \cref{fig:results:fit_GaAs30kV}, the 12th order polynomial has a lower RMS error and is thus preferred.
Further, the model must be fitted.

\begin{quote}
    \verb|m.fit()|

    % \verb|m.fit_background()| % not neccessary

    \verb|m.plot()|
\end{quote}

The first line fits the model to the data to the components and the second line plots the model.
HyperSpy have an own option for fitting only the background.
Since the background is one of the components in m, it is fitted with the code line above.


\subsection{Quantification after model fitting}
\label{sec:discussion:steps:quantification:model}

\begin{quote}
    \verb|m_i = m.get_lines_intensity()|

    \verb|k_factors = [<k-factor 1>, <k-factor 2>] |

    \verb|quant = s.quantification(s_i, method='CL', factors=k_factors)|

    \verb|print(f'E1: {quant[0].data[0]:.2f} \%, E1: {quant[1].data[0]:.2f} \%')|

\end{quote}

The quantification after model fitting is done in the same way as in \cref{sec:discussion:steps:quantification:linear}, but with intensity from the model instead of the signal.
When modelling GaAs, the model can add the intensity from both K-lines and L-lines.
Since AZtec only gives the k-factors for either the K-lines or the L-lines, the user must remove the lines without k-factors before quantification.
As explained in \cref{sec:discussion:general:intensities}, overlapping peaks need to be modelled to get the correct intensities.


\subsection{Calibrating the spectrum with the HyperSpy model}
\label{sec:discussion:steps:HyperSpycalibration}

\begin{quote}

    \verb|m.calibrate_energy_axis(calibrate='scale')|

    \verb|m.calibrate_energy_axis(calibrate='offset')|

\end{quote}

The two lines above calibrates the spectrum with the HyperSpy model and updates the dispersion and zero offset.
The metadata in the \verb|signal| s is updated with the new calibration.
Thus, doing the previous step with quantification after model fitting can give a more correct quantification.
This is the HyperSpy calibration that is used in \cref{tab:results:calibration-quantification}.

\section{Peak and background modelling}
\label{sec:discussion:modelling}
% How can the peaks and the background be modelled in a way that is easy to understand?

The next sub-problem was to find out how the peaks and the background are modelled in a way that is easy to understand.
The model was built without HyperSpy, with the idea of making the steps clear for a user.
Once the model is fitted to the data, it can be used to calibrate and quantify the peaks and background.
This can be done by comparing the peak ratios, i.e. the relative areas under the peaks, and using these ratios to calculate the Cliff-Lorimer quantification, as shown in \cref{tab:results:ratios}.
In the table the ratio is also multiplied with their respective k-factors, to get the Cliff-Lorimer quantification.

% identyfy peaks with peak finder
% problem: overlapping peaks, eg. Mo
% problem: includes zero peak. Is that an issue?
The first step in creating a model is to identify the peaks.
The peaks are assumed to be Gaussian curves.
Other possible shapes are Lorentzian curves, but that has not been a part of this project.
The initial way of identifying peaks was that the user manually identified the peaks.
Later the peaks were identified with the function \verb|scipy.signal.find_peaks()|, using the argument \verb|prominence=0.01|\footnote{\url{https://en.wikipedia.org/wiki/Topographic_prominence}}.
The prominence is the height of the peak above the background.
As seen in \cref{fig:results:fit_GaAs30kV}, the given prominence identifies many but not all the peaks.
Further work on the peak finder is needed to find a good and robust way of identifying the peaks.
The figure also shows that the peaks are well-fitted by the model, while the background is fitted fairly well.



% make Gaussian and a polynomial
% problem: initial background guess. Solution: clip out the peaks with linear interpolation and fit a polynomial to the rest.
% problem: normalized data. initial guesses are hard on counts, and fitting is slower.
% early problem, now solved: some Gaussians would be put as the background. solution: fit both Gaussians and polynomial
Once the peaks have been identified in the data, the next step in creating a model is to fit a Gaussian curve to each peak and a polynomial to the background.
In order to do this, the model components need initial guesses for their parameters.
For the background, this means providing initial guesses for the coefficients of the polynomial.
A good way to do this is to remove the peaks from the data using linear interpolation, and then fit a polynomial to the remaining data.
The coefficients of the initial fitted polynomial can then be used as the initial guesses for the background in the model.
Each Gaussian is described by its mean, standard deviation, and height, as shown in \cref{eq:theory:empirical:gaussian}.
The mean is the peak position, which has its initial guess from the peak finder.
The standard deviation is the width of the peak, where $\textnormal{FWHM} = \textnormal{std}*2*\sqrt{2*\ln{2}}$\footnote{FWHM defined at: \url{https://en.wikipedia.org/wiki/Full_width_at_half_maximum}}.
The height is the amplitude of the peak.
The easiest way to get the initial guesses for the standard deviation and the height is to normalize the spectrum and set both parameters to 1.
In the normalization the highest peak was set to 1, and the rest of the peaks were scaled accordingly.
With the initial guesses, the whole model is ready to be fitted.

% why normalize
Normalization is a technique that is often used in data analysis to make the fitting process faster and more stable.
In the case of this project, it was found that normalizing the data was necessary in order to avoid failure of the SciPy fitting algorithm.
This failure is likely due to the fact that \verb|scipy.optimize.curve_fit()| is optimized for certain types or ranges of data, and normalizing the data allows it to handle the data more effectively.
It is important to note that although the data is normalized for the purpose of fitting, the quantification of the peaks is based on the ratios between the peaks, which are not affected by the normalization.
This means that the normalization does not affect the accuracy of the quantification, but only the speed and stability of the fitting process.
However, the perceived spectrum is affected by the normalization, which is important in the qualitative analysis.
In this project, different normalization techniques were explored in order to find the one that provided stable results with SciPy, while still allowing the qualitative analysis to be performed.

% the selected normalization
The normalization that was used in the end was to set the highest peak to 1, and scale the rest of the peaks accordingly.
This normalization, with some scaling and cropping of the figure, allowed the peaks to be visible and comparable, while still allowing the qualitative analysis to be performed.
No special features were observed to be lost by using this normalization.
Some EDS users and textbooks prefer to use the logarithmic scale on the y-axis instead of normalizing the data.
The logarithmic scale is a good way to make the spectrum easier to read, but it does not affect the fitting process.
In addition, plotting with a logarithmic scale was tested while doing the qualitative analysis, but it was found that the logarithmic scale provided less detailed information on the smaller peaks and strays than the linear scale.
In the initial AZtec plot in \cref{fig:GaAs30kV_AZ} the spectrum is plotted both with a linear and a logarithmic scale.
Using the logarithmic scale is in the end a user preference.
Thus, the linear scale with normalization was chosen as the best way to visualize the spectrum for the qualitative analysis, and as the best input for the fitting process.


% curve fit
% problem: sometimes fails
With the model components ready and the data normalized, the model can be fitted to the data.
The \verb|curve_fit()| function uses the Levenberg–Marquardt algorithm to fit the model to the data, and returns the optimal fitted parameters.
Fitting both the Gaussians and the background at the same time makes the fitting more stable.
One of the first iterations, where the user manually inputted the peaks, the fitting tended to partially fail.
The issue was that the fitting only was done on the peaks.
To minimize the error in the fitting, one of the Gaussian curves with a low amplitude was moved and got a huge standard deviation, which compensated the background.
This was fixed by fitting both the Gaussians and the background at the same time.
Doing this made the fitting both better, and it failed less often.

% problem with severely overlapping peaks
The issue of severely overlapping peaks was encountered when fitting the Mo spectrum.
The Mo L$\alpha$ and L$\beta$ peaks are very close to each other, and even though the fitting algorithm would handle this, the peak finder would not recognize the overlapping peaks as two separate peaks.
This problem can be fixed manually, but with the limited time in this project it was categorized as an edge case that would not be handled.

\ton{Now I am happy with the discussion until this point (13.12.22). How would you rate it? Any suggestions on the bigger picture of the discussion? I do need to wrap it up soon, because I still need to fill in some theory, fix the conclusion and the introductions.}








%
\section{Calibration}
\label{sec:discussion:calibration}
% How is the spectrum calibrated, and is AZtec different than HyperSpy?

\brynjar{Try not to repeat the same things as earlier.}

The next sub-problem was to calibrate the data with a self produced Python script.
With a fitted model of the spectrum, the calibration can be done.
Calibration can both be done on raw data with channels on the x-axis and on poorly calibrated data with energy on the x-axis.
The dispersion is calculated with \cref{eq:theory:calibration:dispersion}.
Table \cref{tab:results:calibrations} shows calibration from AZtec, HyperSpy, and the self produced Python script.


% Something about how the calibrated peak of Ga K$\alpha$ is directly on K$\alpha_1$.




% Why I selected the Ga La and As Ka peaks for calibration.
% Less extrapolation. Peaks are far apart.
% The peaks need a good Gaussian fit.
% Need a nice curve.
% High peak to background.
% Should be a sample that is easily available. Cu tape would be nice, since it is in all labs and samples.
% Mo is far apart, but looks bad.
% Mo is also harder to fit automatically, because of the overlapping peak.



%
%
\section{Background models}
\label{sec:discussion:background}
% How does different background models affect the quantitative analysis done in HyperSpy?

\brynjar{Try not to repeat the same things as earlier.}


The next sub-problem was to find out how different background models affect the quantitative analysis done in HyperSpy, and how well different order polynomials fit the background.
The background models were tested on the spectrum of GaAs, and later also on \brynjar{TODO: other spectra. Also make a table here with results}.
The background was modelled as a polynomial of different orders.
To quantify the different background models, the residuals were calculated.
The residuals are the difference between the data and the model.
\brynjar{Use root-mean-square error?}
The TABLE XXXX \brynjar{make table} shows the residuals for the different order background models.
The best orders were visually inspected.
A later idea was to model the background as a spline, which is a piece wise polynomial.
The spline is a piece wise polynomial with a smooth transition between the pieces.
The spline was not tested in this project, but it could be a good alternative to the polynomial background model.

% TODO: Try the background as a spline, which is a piecewise polynomials.

% conclusion: try splines, but then what to do under the peak?

%
%
\section{Analysis failure}
\label{sec:discussion:failure}
% When does the analysis fail, both in AZtec and HyperSpy?

The next sub-problem was to find out when the analysis fails, both in AZtec and HyperSpy.
% quantification with 








\section{Choices in HyperSpy}

% Using EDS_TEM since EDS_SEM does not have all these functions.
% Commercial packages do quantify SEM signal, and they are kinda good at it.
% Should be more accurate on NW sample, but is it?
% Reference the discussion on GitHub?





