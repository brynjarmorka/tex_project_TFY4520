\chapter{Discussion}
\label{chap:discussion}


% % Ga Ka where Ka2 is not visible
% % point of the paragraph: empirical view is good enough
% In \cref{fig:theory:GaAs30keV-K-lines} the calibrated peak of Ga $K\alpha$ is directly on $K\alpha_1$ and the $K\alpha_\textnormal{HyperSpy}$, the emission from $K\alpha_2$ is missing.
% The energy difference is too small to differentiate the two peaks in EDS, but the peak has no left-shift, implying that the peak is not a mixture of $K\alpha_1$ and $K\alpha_2$.
% The peak shape is \brynjar{calculate the shape}, which is close to a perfect Gaussian.
% There are possible theoretical explainations for the lacking emission from $K\alpha_2$.
% One explaination is the quantum mechanical effect called \dots, where two close lines appear as one stronger line.
% This effect is not only present in EDS, but also in \dots.
% Another explaination is that Ga $K\alpha_2$ could also be weaker than $K\alpha_1$, but there is no reason for this to be the case.
% The take away from this is that the empirical view tends to be good enough for EDS analysis.


The discussion is presented in this chapter.
Producing code from scratch is both a time-consuming process and a learning process.
While developing the code, the author learned a lot about EDS analysis and  ideas emerged for how EDS analysis could be improved.
This chapter begins with an initial discussion of the qualitative results, and then it connects the sub-problems to the results and work done in the project.
The order of the sub-problems is the same as in \cref{chap:introduction}, which is based on the work process.
The chapter ends with a discussion of the \hyperref[mainproblem]{Main problem~\ref*{mainproblem}}.


















% Pasted from the results:

\subsection{General results from the spectra}
\label{sec:discussion:qualitative:general}

All the spectra have peaks with high peak-to-background ratio. %discuss: sampling good enough? I do not know if i miss some peaks, but the ones i have are nice for qualitative analysis at least.
% The zero peak from the Oxford detector is visible in all the spectra as the first peak, with a center at 0.00924 keV.
In all the spectra, the highest peak is below 5 keV. % discussion: what peaks give the highest counts, with theory and empirical data. detector efficiency? Absorption? Overvoltage? etc
The zero peak starts before 0 keV and has its maximum at 0.008 keV. % discuss: could this be part of the calibration error in AZtec?
The GaAs, NW and Mo spectra show clearly that the peaks broaden with higher E, since they have peaks at low and middle to high energy. % discuss why this happens.
The width of the peaks are quantified with FWHM in the quantitative section below.
When doing the qualitative analysis, it became clear that the FIB stub was not made of Fe as expected, but rather of Al with a peak at 1.48 keV.
All the 5 kV spectra decrease more or less linearly from 1 to 5 keV. % discussion: overvoltage
% Another discovery was that the Cu-tape does not give a good Cu signal.
% The high peak in the Cu-tape spectra at 0.260 keV are from C and not the Cu L$\alpha$ peak.
% Only the Cu-tape taken at 30 kV has a Cu peak, but it is very small.
Even though the Cu spectra at 30 kV has the Cu K$\alpha$ peak, the Cu L$\alpha$ peak is completely missing. % this is very, very strange. ! TODO: Discuss
\brynjar{Add transition sentence.}

% double peaks, ie peaks which are overlapping
Some peaks in the spectra are overlapping, which shifts the shape of the peaks.
An example of this is the As K$\alpha$ peak and the Ga K$\beta$ peak in the NW and the GaAs bulk wafer spectra.
These peaks are overlapping, but also far enough apart that the peaks are still distinguishable.
Another example of overlapping peaks is the Mo L$\alpha$ peak and the Mo L$\beta_1$, which are overlapping so much that they are hard to distinguish.
Since they are harder to distinguish, the peak fitting makes one Gaussian for the two peaks, which is off on both peak centers. % for discussu\ion: which is why the deviation of the Mo L$\alpha$ peak is off in the calibration accuracy table, \cref{tab:results:calibration-peak-accuracy}.
Overlapping peaks makes counting the signal from specific peaks harder.
\brynjar{Figure of double peaks?}

% background, 
% bg increase with lower kV, 
% bg shape, 
The signal from the background is another factor which makes counting more difficult.
% TON comment: 
%   Need to account for or fit the background, to be able to substract it and get right peak counts.
%   Could keep these comments here, but more clear to split results and discussion/interpretation.
%   Think how this representation and later discussion will link to set aim of the work ("better EDX").
In general, background in the acquired spectra is low, but with different shapes.
In the GaAs, Si and Mo the height of the background decrease with higher acceleration voltage.
The background in the Cu spectra increase with higher acceleration voltage.
The most similar background signal over different voltages are in the NW spectra. % discuss: thin sample -> less background?
The values of the background radiation is in general very low and almost flat above the highest peak in the spectra. % discussion: nothing to reduce the speed of the e-
% For example, the Cu 10 kV spectrum have its highest peak at 0.5 keV, and the background is almost zero above 0.5 keV. not the best example
The Al spectrum background is high before the high Al K$\alpha$ peak, and much lower after the high peak.
Both the value and the shape of the background is different before and after the peak.
The same behavior is clearly true in the Si spectra.
% TON comment:
% yes, it is absorption at that energy.
The peak where the background change is the highest peak, Si K$\alpha$.
The background values in Si 30 kV are 10 times higher before than after the highest peak.
The background shape in Si 30 kV is almost linear from 0.6 to 1.6 keV, drops to 10\% height from 1.6 to 1.9 keV, and then follows the expected background shape from 1.9 keV.
The expected background shape is illustrated in \brynjar{Make a drawing of the background.} % discussion: explain that the bg is dependent on a peak (ie material) being present. This makes fiting hard, since there are some peak-dependency which is hard to predict.
%discussion: model the peak as more than one polygon added together, since the shape changes. For further works. That is called spline.
All the other spectra show the same behavior with their highest peak and the peaks effect on the background as the Al and Si spectra.
In general, the background signals are low, but their different shapes and heights makes it harder to fit the peaks of the characteristic X-ray lines.
\brynjar{Figure of background? And figure of fit of background before and after a tall peak?}
% \ton{The last sentence is meant to be a transition/finishing sentence, but might be too much discussion.} 
% TON Answer, when this was results:
% OK overall summary of results. OK for me. Discussion would be on why shapes are different and not follow the formula that describes the background so well..
% discussion: make the background as a spline. But then, what to do under the peak?



% discussion: understanding the strays properly is actually helpful for the qualitative analysis, since some elements can be excluded based on the strays and some strays are only present in certain materials.
% Si stray in all spectra
% stray in NW outside beam, Mo, Cu and Sb


% TON There are many different strays. SOme were mentioned above. This paragraph is short. Maybe better to have it as a header in discussion?


In addition to the characteristic peaks and the background, there are also artifacts and strays in the spectra. % discussion: lower peaks are super important, eg in the Al spectra where it is small amounts of Mg and Mn, probably. Asserting what is a stray and what is characteristic is important for quantification.
All the spectra have signal from the K$\alpha$ line of C and O, which could be contamination and oxide layers. % discussion: this is contamination. Carbon deposited from the beam, visisble on the Cu-tape images. Oxide layers? If yes, look at material properties.
The C and O signal is higher at lower acceleration voltage.
All the spectra have a Si peak at 1.74 keV. % This is the Si escape peak for some spectra, but not all.
For all but the Si spectra, the Si K$\alpha$ peak at 1.74 keV is a stray from outside the beam or the Si escape peak from the detector.
Some spectra have additional signals from elements outside the area of the main beam, like the Mo, Sb and Cu peak in the NW spectra. % pretty sure it is Sb, because of the series of peaks at 3.6, 3.8, 4.1 and 4.3. 
% discuss both interactin volume and bonus peaks from X-ray strays in the chamber/sample.
The Sb peaks in the NW spectra are at 3.60, 3.85, 4.10 and 4.35 keV, being the L$\alpha_1$, L$\beta_1$, L$\beta_2$ and L$\gamma_1$ peaks.
All four Mo spectra have a peak at 0.175 keV, which match best with B K$\alpha$ at 0.183 keV.
\brynjar{Figure of strays?}
\brynjar{Finishing sentence for the general observations.}


% Sum peaks, to discussion
% The Si spectra have a sum peak at 3.49 keV, which is the sum of Si K$\alpha$ at 1.74 keV. % discussion: this could be Sb, but not like NW where Sb is a series of peaks. Also say something about the resolution of the detector, which is higher at lower E. And also that Sb cannot be excluded bc. of the K peak. And also that the sum peaks can be tested with low DT. And something here about the bad calibration in AZtec which might make this sum peak as a Sb peak, but I now know that it is a Si peak sum peak and not a Sb peak.
% The Al spectrum have its highest peak at 1.48 keV and a sum peak at 2.98 keV
% The Mo spectrum have a sum peak at 4.65 keV, which is the sum of Mo L$\alpha$ at 2.293 keV and Mo L$\beta_1$ at 2.395 keV. % same shape as Mo La  Mo Lb1, i.e. also Mo La+MoLa sum
% The GaAs spectrum at 30 kV have two small sum peak signals at 18.5 and 19.5 keV, while the NW spectrum at 30 kV with lower DT does not have these peaks. % discussion: results show that sum peaks are lower with lower DT

% Mo double peak
% GaAs double peak
% Si stray on Cu
% also a sum peak at Al 30 kV spectrum (look at DT)
% Si double count at 3.4 keV which could be Sn (but the E resolution could seperate them Ton thinks at this low keV and Sn would have multiple peaks, cannot exclude bc of the K peak could count with low DT to minimize double counts consequence of AZtecs bad calibration)






















%
%
\section{Analysis steps in HyperSpy}
\label{sec:discussion:steps}
% What are done with the data at the different steps in the analysis when using HyperSpy?

\hyperref[subproblem1]{Sub-problem~\ref*{subproblem1}} was to do qualitative analysis of the GaAs wafer in AZtec and HyperSpy.
The analysis was done as an out-of-the-box analysis, i.e. just following the steps in the documentation.
AZtec has a GUI for analysis, but it is not possible to see what is done with the data at the different steps.
HyperSpy have documentation online, and the following subsections explain how the analysis was done in HyperSpy.

% The next sub-problem was to find out what is done with the data at the different steps in the analysis when using HyperSpy.
% In these steps it is assumed that the user have done qualitative analysis and want to do quantitative analysis on a set of elements.
% The analysis in AZtec is done as a black box, so it is not possible to see what is done with the data at the different steps.
Each subsection starts with some code lines, followed by an explanation of what the lines do.
All variables inside crocodile need to be set by the user, e.g. \verb|<element_list>| would be set to \verb|['Ga', 'As']| for the GaAs wafer.
An example notebook with quantification of the GaAs wafer is attached in APPENDIX.
\brynjar{Make a notebook with GaAs quantification in HyperSpy, with the data somehow.}

\subsection{Loading the data and specifying the elements}
\label{sec:discussion:steps:load}
\begin{quote}
    \verb|s = hs.load(<filepath>, signal="EDS_TEM")|

    \verb|s.set_elements(<element_list>)|
\end{quote}

The first step in the analysis is to load the data as a HyperSpy \verb|signal| type, and specifying the signal as TEM.
The \verb|signal| type is a class in HyperSpy that contains the data and the metadata, and it has methods for analysis.
The \verb|signal| type must be specified as TEM, because the \verb|signal| type for SEM is very limited and does not have a method for quantification.
When using .emsa files from AZtec, as is done in this project, the metadata contains some relevant and some irrelevant information.
The information relevant later in this project is:
acceleration voltage, dispersion, zero offset, energy resolution Mn $K\alpha$,
After loading, it is possible to plot the data with \verb|s.plot()|.

Already at this point there is a big problem with using HyperSpy on the acquired data: the analysis methods are not implemented for SEM but for TEM.
As explained by Skomedal in her master's thesis \cite{skomedal_improving_2022}, the quantification with the Cliff-Lorimer method of TEM EDS data use approximations which are valid for thin samples.
The approximation is that the sample is thin enough to ignore absorption and fluorescence, which is a very crude approximation to use on bulk SEM EDS data.
Still, the results show that it is possible to quantify the elements in the GaAs wafer with the Cliff-Lorimer method and get plausible results, but also that the analysis breaks down occasionally. % Copilot: but the results should be taken with a grain of salt.
Implementing quantification from the SEM type signal in HyperSpy have been discussed on the repositories GitHub issues page\footnote{\url{https://github.com/hyperspy/hyperspy/issues/2332}}.
The topic have been discussed in 2020, 2017 and 2015.
People have been working on quantification of SEM EDS data, but the work have not been finished yet.

The quantification done with the CL method in HyperSpy on the three different calibrations in \cref{tab:results:calibration-quantification} are all within 10\% of the expected composition.
Carter and Williams \cite[p. 612 and 648]{carter2016transmission} state that the accuracy of EDS is at best $\pm$ 3-5\%, but that it could be reduced to around $\pm$ 1.7\% with very long acquisition times and a careful analysis.
Carter and Williams claim that quantitative errors less than $\pm$ 5-10\% takes a lot of time and effort, and compositions with error below 5\% should be regarded extra carefully and with suspicion.
The best result, i.e. the result closest to 50\%, from the HyperSpy CL quantification is strangely from the AZtec calibration with a 6.25\% error.
It is strange that the AZtec calibration is the best, because that is the calibration which misses most on the line accuracy in \cref{tab:results:calibration-peak-accuracy}.
This implies that the calibration is not the most important factor for the quantification, at least not when the calibration is around $\pm$ 1\%.
The different calibrations are discussed further in \cref{sec:discussion:calibration}.
In the 30 kV GaAs sample the signal-to-noise ratio is good, the sample time was long and the peaks are well resolved, which make the input data good and allows for better quantification.
The CL method is known to work on SEM data, but normally one would also do the ZAF correction, which is described in chapter 19.10.3 of Goldstein \cite{goldstein_scanning_2018}.
The ZAF correction adjust the signal for the atomic number effect, the absorption in the bulk of the sample and the X-ray fluorescence of new lines with lower energy than the initial characteristic X-ray.
Since Ga and As are number 31 and 33 in the periodic table, it could be that the ZAF corrections are small and would not have a big effect on the results.
When doing the quantification it was tested to include O and C, but that changed the results from plausible to completely wrong.
The quantification analysis also broke down when using the 10 and 15 kV spectra, but the reason for that was not clear.
The breaking down of the analysis is discussed further in \cref{sec:discussion:failure}.


Results in this project show that the CL method in HyperSpy for TEM EDS data sometimes yield plausible results on SEM EDS data and sometimes break down, thus the method should be used with caution.
The goal of this project was not to do or to implement cutting edge quantification of EDS data from a SEM sample, but rather to understand the analysis steps and what factors that could affect the results.
Thus, using the quantification method from the TEM signal type was regarded as good enough for this project.





\subsection{Removing the background linearly}
\label{sec:discussion:steps:background}
\begin{quote}
    \verb|bw = s1.estimate_background_windows(windows_width=<number>)|

    \verb|iw =  s1.estimate_integration_windows(windows_width=<number>)|
\end{quote}

The next step is to remove the background, which with the above code is done by a linear fit.
The background can be removed through model fitting, which is covered in \cref{sec:discussion:steps:model_fitting}.
The variable \verb|windows_width| sets how wide the windows are for the background and integration, measured in FWHMs.
A good starting value for \verb|windows_width| is 2, but it should be tested by the user with a plot to see if the background will be removed correctly.
The estimated windows can be plotted with:

\begin{quote}
    \verb|s.plot(xray_lines=True, background_windows=bw, integration_windows=iw)|
\end{quote}


Plotting has proven to be a very useful tool for understanding what the different steps in the analysis do.
Unfortunately, the implemented plotting in HyperSpy is limited.
For example, when plotting a modelled spectrum with Ga and As, the model contains all the peaks in the HyperSpy library and all the peaks are plotted, but it is not possible to see what the different peaks are.
In other words, the plot of a model can show all the independent components, but adding a legend is not an option.
Another limitation of the plotting is the interactivity when saving the plot.
Matplotlib requires the code to run in the background for it to be interactive, while Plotly can save the plot as an interactive HTML file for later inspection.
One of the advantages with the plotting in HyperSpy is how accesable it is to plot in the different steps, which the user can use to understand and verify the analysis.
The HyperSpy plotting was probably not made with the intention of being used for publication, but rather for quick and easy plotting of the data.
If that is the case, the plotting serves its purpose well and has proven to be very useful for this project.
% Copilot did this: One of the authors of HyperSpy, Tomas Ostasevicius, has been working on a new plotting library for HyperSpy, which will be released in the next major release of HyperSpy.

One of the more helpful insights from plotting in this project was that removing the background is not trivial.
This is especially true for background signals modelled as a polynomial.
Linear background removal works well for some peaks, for example the K-lines of Ga and As, but not for the L-lines.
The background around the K-lines are more linear, because of the exponential decay of the background.
Around the L-lines the background is affected by the absorption of the Bremsstrahlung lower than ~2 keV and the observation of higher background levels below the highest peak in the spectrum.
The background removal is discussed further in \cref{sec:discussion:background}.




\subsection{Quantification after linear background removal}
\label{sec:discussion:steps:quantification:linear}

\begin{quote}
    \verb|s_i = s.get_lines_intensity(background_windows=bw, integration_windows=iw)|

    \verb|k_factors = [<k-factor 1>, <k-factor 2>]  |

    \verb|quant = s.quantification(s_i, method='CL', factors=k_factors)|

    \verb|print(f'E1: {quant[0].data[0]:.2f} \%, E1: {quant[1].data[0]:.2f} \%')|
\end{quote}

The quantification is done with the four lines of code above, where the last one prints the results.
The first line gets the intensity of the peak corresponding to the lines of the specified element.
HyperSpy selects automatically which lines to use for quantification.
To see which lines are used, the \verb|s_i| variable can be printed.
The second line sets the k-factors.
The k-factors in this project have been the one from AZtec, which are theoretically estimated.
The third line does the quantification, where the method is specified.
The method is the Cliff-Lorimer method, described in detail in Mari Skomedal's master thesis \cite[Sec. 2.2.3]{skomedal_improving_2022}.
HyperSpy has a method for quantification with the zeta factor method.
The zeta method requires the value for the beam current, which was not measured in this project. \footnote{Results from the zeta method can be converted to the cross section method, see the page "EDS Quantification" in the HyperSpy documentation.}
% \footnote{Results from the zeta methon can be converted to the cross section method, see \url{http://hyperspy.org/hyperspy-doc/current/user_guide/eds.html#eds-quantification}.}


% k-factors
The k-factors are essential for the Cliff-Lorimer quantification, and the k-factors listed in \cref{tab:results:k-factors} give some insight into how AZtec calculates the k-factors.
From the results it is clear that AZtec at least adjust the k-factor for element and for voltage.
It could be that factors like the beam current and time live could affect the k-factor, but this was not investigated in this project.
The L-line k-factors are start at 1.1 for 5 kV and increase to 1.2 for 15 kV.
The As k-factor for the L-line at 5, 10 ad 15 kV is 11\%, 7\% and 6\% greaten than for Ga.
The K-line k-factor are 3.3 for Ga and 4.2 for As, implying either a high sensitivity for higher voltages or two different models for calculating the k-factors.
Another possibility is that the K-lines and L-lines have a seperate model for calculating the k-factors.
This can be tested by extracting the k-factors for the K-lines from the 15 kV spectrum, as the 15 kV spectrum do have peaks for both the K-lines and L-lines.
The k-factors for Ga are in all four cases higher than for As, which makes sense, as all the Ga lines are higher and have more counts than the As lines.
Quantification of the ratio between the Ga and As peaks are dont in \cref{tab:results:ratios}, where it is clear that the k-factor push the ratio towards 1.
The peak ratio times the k-factor for 5 kV is 1.1, and the next closest to 1 is 1.3 for 30 kV.
This result imply that the theoretically calculated k-factors could be better for high and low voltages.


\ton{I am happy with the discussion untill this point (08.12.22). How would you rate it?}


% double peaks in sum, why to use modeled peaks and not raw counts.


\subsection{Removing the background with model fitting}
\label{sec:discussion:steps:model_fitting}
Another way to remove the background is to fit a model to the data.
This step would be done right after loading the data.
If the raw data contains a zero peak, as is the case for most Oxford instrument EDS detectors, the zero peak needs to be removed before fitting the model.
The zero peak is removed by skipping the first n channels, where n=30 works well with the data from the GaAs wafer.
The model fitting is done with the following code:

\begin{quote}
    \verb|s = s.isig[<zero_peak>:]|

    \verb|m = s.create_model(auto_background=False)|

    \verb|m.add_polynomial_background(order=12)|

    \verb|m.add_family_lines(<list_of_element_lines>)|

    \verb|m.plot()|

\end{quote}

The lines above removes the zero peak, create a model from the \verb|signal| s, adds a 12th order polynomial, add the lines of the elements in the \verb|signal|, and plot the model.
This model is not fitted, it is just a generated spectrum with the lines of the elements.
Eventually, the method \verb|create_model()| can take the boolean argument \verb|auto_add_lines=True|, which will automatically detect the elements in the sample.
The model consists of a number of components, which can be accessed with \verb|m.components|.
The components are all the Gaussian peaks in the spectrum, in addition to the background as a 12th order polynomial.
The order of the polynomial can be changed, but it should be tested by the user to see if it is a good fit.
Further, the model must be fitted.

\begin{quote}
    \verb|m.fit()|

    % \verb|m.fit_background()| % not neccessary

    \verb|m.plot()|
\end{quote}

The first line fits the model to the data to the components and the second line plots the model.
HyperSpy have an own option for fitting only the background.
Since the background is one of the components in m, it is fitted with the code line above.


\subsection{Quantification after model fitting}
\label{sec:discussion:steps:quantification:model}

\begin{quote}
    \verb|m_i = m.get_lines_intensity()|

    \verb|k_factors = [<k-factor 1>, <k-factor 2>] |

    \verb|quant = s.quantification(s_i, method='CL', factors=k_factors)|

    \verb|print(f'E1: {quant[0].data[0]:.2f} \%, E1: {quant[1].data[0]:.2f} \%')|

\end{quote}

The quantification after model fitting is done in the same way as in \cref{sec:discussion:steps:quantification:linear}, but with intensity from the model instead of the signal.
When modelling GaAs, the model can add the intensity from both K-lines and L-lines.
Since AZtec only gives the k-factors for either the K-lines or the L-lines, the user must remove the lines without k-factors before quantification.


\subsection{Calibrating the spectrum with the HyperSpy model}
\label{sec:discussion:steps:HyperSpycalibration}

\begin{quote}

    \verb|m.calibrate_energy_axis(calibrate='scale')|

    \verb|m.calibrate_energy_axis(calibrate='offset')|

\end{quote}

The two lines above calibrates the spectrum with the HyperSpy model and updates the dispersion and zero offset.
The metadata in the \verb|signal| s is updated with the new calibration.
Thus, doing the previous step with quantification after model fitting can give a more correct quantification.
%%% THE Calibration gave 1% better result, i think. I need to look at it again.
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%
%
\section{Peak and background modelling}
\label{sec:discussion:modelling}
% How can the peaks and the background be modelled in a way that is easy to understand?

The next sub-problem was to find out how the peaks and the background are modelled in a way that is easy to understand.
The model was built without HyperSpy, with the idea of making every step easier to understand.
The model was used to be able to remove the background and be able to calibrate the spectrum.
The model was compared to the HyperSpy model.
The model could be used to quantify the elements in the sample, but this was not done in this project.
\brynjar{Do I want to do this?}

% identyfy peaks with peak finder
% problem: double peaks, eg. Mo
% problem: includes zero peak. Is that an issue?
The first step in creating a model is to identify the peaks.
The peaks are assumed to be Gaussian curves.
The initial way of identifying peaks was that the user manually identified the peaks.
Later the peaks were identified with the function \verb|find_peaks()| from the \verb|scipy.signal| package.
Different peak prominence were tested, and the peak prominence of 0.01 gave the best results.


% make Gaussian and a polynomial
% problem: initial background guess. Solution: clip out the peaks with linear interpolation and fit a polynomial to the rest.
% problem: normalized data. initial guesses are hard on counts, and fitting is slower.
% early problem, now solved: some Gaussians would be put as the background. solution: fit both Gaussians and polynomial
The second step is to make a Gaussian in each peak and one polynomial for the background.
To do the fitting, the components need an initial guess.
The background needs a coefficient for each order of the polynomial.
Each Gaussian need to have a mean, a standard deviation, and a height.
The mean is the peak position.
The standard deviation is the width of the peak, where $\textnormal{FWHM} = \textnormal{std}*2*\sqrt{2*\ln{2}}$\footnote{FWHM defined at: \url{https://en.wikipedia.org/wiki/Full_width_at_half_maximum}}.
The height is the amplitude of the peak.
The easiest way to get the initial guesses for the Gaussians is to normalize the data and set all three parameters to 1.
In the normalization the highest peak was set to 1, and the rest of the peaks were scaled accordingly.
The best way to get the initial guesses for the background is to clip out the peaks with linear interpolation and fit a polynomial.
The initial guesses for the background are then the coefficients of the polynomial.
With the initial guesses, the whole model is ready to be fitted.

% curve fit
% problem: sometimes fails
The third step is to fit the model to the data.
Using the \verb|curve_fit()| function from the \verb|scipy.optimize| package, the model is fitted to the data.
The function \verb|curve_fit()| uses the Levenbergâ€“Marquardt algorithm to fit the model to the data.
The function \verb|curve_fit()| returns the optimal parameters for the model.
Fitting both the Gaussians and the background at the same time makes the fitting more stable.
One of the first iterations, where the user manually inputted the peaks, the fitting tended to partially fail.
The issue was that the fitting only was done on the peaks.
To minimize the error in the fitting, one of the Gaussian curves with a low amplitude was moved and got a huge standard deviation, which compensated the background.
This was fixed by fitting both the Gaussians and the background at the same time.
Doing this made the fitting both better, and it failed less often.

\brynjar{Issue: fitting e.g. Mo with two clear peaks, but not with enough prominence to be found by the peak finder.}
%
%
\section{Calibration}
\label{sec:discussion:calibration}
% How is the spectrum calibrated, and is AZtec different than HyperSpy?

The next sub-problem was to calibrate the data with a self produced Python script.
With a fitted model of the spectrum, the calibration can be done.
Calibration can both be done on raw data with channels on the x-axis and on poorly calibrated data with energy on the x-axis.
The dispersion is calculated with \cref{eq:theory:calibration:dispersion}.
Table \cref{tab:results:calibrations} shows calibration from AZtec, HyperSpy, and the self produced Python script.


% Something about how the calibrated peak of Ga $K\alpha$ is directly on $K\alpha_1$.

%
%
\section{Background models}
\label{sec:discussion:background}
% How does different background models affect the quantitative analysis done in HyperSpy?

The next sub-problem was to find out how different background models affect the quantitative analysis done in HyperSpy, and how well different order polynomials fit the background.
The background models were tested on the spectrum of GaAs, and later also on \brynjar{TODO: other spectra. Also make a table here with results}.
The background was modelled as a polynomial of different orders.
To quantify the different background models, the residuals were calculated.
The residuals are the difference between the data and the model.
\brynjar{Use root-mean-square error?}
The TABLE XXXX \brynjar{make table} shows the residuals for the different order background models.
The best orders were visually inspected.
A later idea was to model the background as a spline, which is a piece wise polynomial.
The spline is a piece wise polynomial with a smooth transition between the pieces.
The spline was not tested in this project, but it could be a good alternative to the polynomial background model.

% TODO: Try the background as a spline, which is a piecewise polynomials.

% conclusion: try splines, but then what to do under the peak?

%
%
\section{Analysis failure}
\label{sec:discussion:failure}
% When does the analysis fail, both in AZtec and HyperSpy?

The next sub-problem was to find out when the analysis fails, both in AZtec and HyperSpy.
% quantification with 


\ton{Section about normalization too?}





\section{Calibration decision}

% Why I selected the Ga La and As Ka peaks for calibration.
% Less extrapolation. Peaks are far apart.
% The peaks need a good Gaussian fit.
% Need a nice curve.
% High peak to background.
% Should be a sample that is easily available. Cu tape would be nice, since it is in all labs and samples.
% Mo is far apart, but looks bad.
% Mo is also harder to fit automatically, because of the close double peak.



\section{Choices in HyperSpy}

% Using EDS_TEM since EDS_SEM does not have all these functions.
% Commercial packages do quantify SEM signal, and they are kinda good at it.
% Should be more accurate on NW sample, but is it?
% Reference the discussion on GitHub?





